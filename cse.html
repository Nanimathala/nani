<html>
    <head>

    </head>
    <body>
        <p>Google announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.[3] It was developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities, which he believed would allow the algorithm to trump OpenAI's ChatGPT, which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program, which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]

            In August 2023, The Information published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence-powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of semi-retirement to assist in the development of Gemini, along with hundreds of other engineers from Google Brain and DeepMind.[6][8] Because Gemini was being trained on transcripts of YouTube videos, lawyers were also brought in to filter out any potentially copyrighted materials.[6]
            
            With news of Gemini's impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[9] The Information reported in September that several companies had been granted early access to "an early version" of the LLM, which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot.[10][11] On December 2, it reported that Google had delayed Gemini's launch from the following week to January 2024 due to problems with non-English prompts, adding that three launch events had been planned in New York City, Washington, D.C., and California.[12][13]
            
            Launch
            On December 6, 2023, Pichai and Hassabis announced "Gemini 1.0" at a virtual press conference.[14][15] It comprised three models: Gemini Ultra, designed for "highly complex tasks"; Gemini Pro, designed for "a wide range of tasks"; and Gemini Nano, designed for "on-device tasks". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power "Bard Advanced" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[16][15] It was made available only in English.[15][17] Touted as Google's "largest and most capable AI model" and designed to emulate human behavior,[18][15][19] the company stated that Gemini would not be made widely available until the following year due to the need for "extensive safety testing".[14] Gemini was trained on and powered by Google's Tensor Processing Units (TPUs),[14][17] and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.[20]
            
            Gemini Ultra was said to have outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks,[21][14] while Gemini Pro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%.[3][20] Gemini Pro will be made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.[22][23][24] Hassabis further revealed that DeepMind was exploring how Gemini could be "combined with robotics to physically interact with the world".[25] In accordance with the Executive Order 14110 signed by President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]
            
            Technical specifications
            The three Gemini models share the same software architecture. They are decoder-only Transformers, with modifications to allow efficient training and inference on TPUs. They have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger Gemini models, designed for use by edge devices such as smartphones.[26]
            
            As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order, allowing for a multimodal conversation. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual, consisting of "web documents, books, and code, and includ[ing] image, audio, and video data".[26]
            
            Reception
            </p>
    </body>
</html>